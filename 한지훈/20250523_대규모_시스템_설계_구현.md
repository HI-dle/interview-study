# 대규모 시스템 설계, 구현

---

 - 대규모 시스템에 맞는 설계를 구현할 때 기준과 기술 스택 사용 시 사용 이유에 대해, 아키텍처 구성과 구현에 대해 알아보고자 한다.



 - 대규모 시스템 설계 시 대규모 시스템이란 기준 자체가 맥락에 따라 다르지만 어느 정도 기준을 세우고 시스템 설계를 할 수 있다.



1. 트래픽

   - 요청 수 (RPS: Requests Per Second) : 수백~수천 RPS 이상 (ex. 1,000 RPS 이상)

   - 동시 사용자 수 (Concurrent Users) : 수천~수만 명 이상



2. 데이터 규모

   - 데이터베이스 테이블의 row 수 : 수천만~수억 건 이상

   - 일일 데이터 증가량 : 수 GB~수 TB 이상



3. 인프라 규모

   - 서버 수 : 수십 대 이상의 서버 운영 (웹 서버, DB 서버, 캐시 서버, 메시징 서버 등)

   - 마이크로서비스 개수 : 10개 이상 분산된 MSA 구성



4. 배포 및 CI/CD

   - 무중단 배포, 롤백 체계, 블루-그린/카나리 배포 전략 필요





 - 수천 RPS 이상, 수억 건 이상의 데이터를 처리하고, 분산 시스템 또는 MSA 구조로 운영되며, 장애 대응과 트래픽 스케일링이 가능한 시스템을 대규모 시스템이라 말한다.


---

 - 시스템 대략적이게 개인적으로 구성해보았다.



시스템으로 대략적으로 금융이나 커머스로 설계한다고 하고 웹 서버 기준으로 백엔드만 구성한다고 가정하고 진행한다. 

백엔드 서버는 자바 스프링 사용할 것이다. 그렇다면 서버 구성은 대략적으로 진행한다 할 때 MSA 구성으로 유레카를 사용 이유는 스프링에서 제공하는 MSA 서버 구성 권장 값이고 지원하는 라이브러리와 프레임워크가 많기 때문에 선택했다.


서버 구성은 간략히 커머스 기준 유레카 서버 1, 주문, 상품, 결제, 유저, 게이트웨이, 재고 이렇게 구성한다. 

각 서비스들은 별개 서버이며 증설은 트래픽에 맞춰 점차적 증설할 계획 유저와 주문은 같은 DB를 서버를 사용한다. 

나머지 상품, 결제, 재고는 별도의 DB 서버를 사용하고 DB는 postgresql을 사용한다. 별도의 DB 서버를 사용하는 서비스는 row 수가 많고 IO가 많을 거라 고려되기 때문이다.

각 서비스끼리 통신을 실시간으로 병렬 처리를 위해, 유실되는 메시지에 대해 failover 고려, cdc 사용 시 다베지움 지원 등의 이유로 데이터 메시지 큐를 카프카 선택 각 브로커나 메시지 큐는 시스템 상태에 따라 결정, 각 데이터 캐싱과 임시 데이터 저장소 사용으로 레디스를 사용 이유는 단일 스레드로 동작하기 때문에 빠른 성능과 redission 같은 라이브러리 지원 및 루아 스크립트 사용으로 한 단위 명령을 스크립트로 동작하게 가능한 이유로 채택했다.

이로인해 트래픽이나 데이터 규모에 대한 부분은 성능을 잡을 수 있다. 이외 모니터링 구성으로 그라파나와 프로메테우스 선택 이유는 데이터 통합이 액추에이터를 통해 잘 이루어져 있고 시각화나 알림에 대해 사용해 본 적 있는 스택이기 때문 코드 품질은 깃을 통해 CI로 관리 배포는 깃 CD를 통해 배포 자동화를 이루어 내고 배포는 안정성을 위해 무중단 배포로 블루 그린 선택으로 아키텍처를 구성



 - 위 설계를 진행할 때 부족하다 생각한 부분은 데이터베이스 선택 이유에 대한 부분과 조금 더 개선할 부분이 있을까 생각이 돼서 AI를 통해 조금 더 디밸롭 받아 보았다.



DB 선택 이유에 대해 부족함을 느꼈고 DB로 Postgresql을 선택한 이유를 설명하기는 다음과 같다.

 - 다양한 인덱싱 방법(GIN, GiST 등)을 통한 검색 성능 최적화, 파티셔닝, 병렬 쿼리 처리, JSON 지원 등 복합 쿼리와 구조화된/비정형 데이터까지 유연하게 처리할 수 있음

 - Debezium과 같은 CDC 도구와의 연동성이 뛰어나 MSA 환경에서도 실시간 데이터 스트림 처리에 유리함

 - PostGIS, TimescaleDB 등 다양한 확장 기능도 고려 가능하여 향후 기능 확장도 용이하므로 상품/재고/결제와 같이 데이터 규모 및 IO가 많은 서비스에 적합한 선택이다.



이외 시스템 설계로 추가할 수 있을만한 부분으로는 다음과 같다.

| 항목                     | 고려사항                                                                 |
|------------------------|--------------------------------------------------------------------------|
| Config Server 고려        | MSA 구성 시 공통 설정을 중앙화하는 Spring Cloud Config Server 도입으로 운영 편의성 증가 |
| API Gateway 보안         | 게이트웨이에서 JWT 검증, Rate Limiting, 인증/인가 분리 등 보안 로직 처리 필요            |
| Kafka 운영 고려사항      | 파티션 설계, 컨슈머 그룹 전략, 메시지 유실 방지(Retry, DLQ) 정책 설계 필요              |
| Redis 이중화             | Redis Sentinel 또는 Redis Cluster를 통한 장애 복구 구조 고려                        |
| 트래픽 관리              | API Gateway + Spring Cloud LoadBalancer 외에도 HPA, Throttling, Circuit Breaker(Hystrix 또는 Resilience4j) 고려 가능 |
| Dev/Stage/Prod 환경 분리 | `application-dev.yml`, `application-prod.yml` 등 설정 파일 별도 분리 운영               |
| 로깅/트레이싱            | ELK Stack 또는 Grafana Loki + Zipkin/Jaeger를 활용한 분산 로깅 및 트레이싱 고려          |









 - 한번 그려보았다. 내부적인 시스템 데이터 버스에 의해 이동하는 거나 요청 흐름 같은 경우는 더 세부적인 설계도가 필요하겠지만 전체적인 시스템 아키텍처는 다음과 같다.
![system.png](image%2Fsystem.png)


 - 간략히 postgresql에 인덱싱 GIN, GiST와 확장에 대해 간략히 알아보고 넘어가 보자.

| 항목              | GIN (Generalized Inverted Index)                              | GiST (Generalized Search Tree)                        |
|-------------------|---------------------------------------------------------------|--------------------------------------------------------|
| **특징**          | 역색인(Inverted Index) 방식                                    | 범용 트리 구조                                           |
| **주로 사용되는 타입** | 배열, JSONB, Full-text 검색                                     | 공간 데이터, 범위 데이터, 유사도 검색                         |
| **적합한 사용처**    | - JSONB 속성 검색<br>- 배열 안의 원소 검색<br>- 텍스트 검색 (to_tsvector) | - PostGIS 공간 검색<br>- 범위 쿼리 (int4range, tsrange)<br>- KNN 검색 |
| **인덱싱 속도**     | 느린 편                                                      | 빠른 편                                                  |
| **검색 성능**      | 매우 빠름 (다수 키 검색에 유리)                                 | 특정 범위/공간/거리 기반 탐색에 강력                         |
| **예시**          | `CREATE INDEX idx_tags_gin ON product USING GIN(tags);`       | `CREATE INDEX idx_location_gist ON store USING GiST(geom);` |



 - GIN은 여러 키워드 중 포함 여부 검색에 강하고 GiST는 거리/위치/범위 검색에 강하다.



### PostGIS란?

PostgreSQL을 공간 데이터베이스로 확장하는 GIS(Geographic Information System) 확장 모듈

위도/경도, 도형(점, 선, 다각형 등) 저장 가능 (geometry, geography 타입)

거리 계산, 포함 여부(ST_Contains), 교차 여부(ST_Intersects) 등 다양한 공간 연산 지원

GiST 인덱스로 성능 향상 가능

````sql
//서울 좌표에서 반경 1km 내의 매장 검색
SELECT * FROM store
WHERE ST_DWithin(geom, ST_MakePoint(126.9784, 37.5665)::geography, 1000);
````

### TimescaleDB란?

시계열 데이터에 특화된 PostgreSQL 확장

대용량 시계열 데이터의 저장, 압축, 집계에 최적화

시계열용 하이퍼테이블(Hypertable) 구조 제공

시간 기준 자동 파티셔닝

속도 향상된 집계, 롤업, 압축 기능

Grafana와도 연동 쉬움
````sql
//5분 단위 평균 온도 집계
SELECT time_bucket('5 minutes', time) AS bucket, avg(temp)
FROM sensor_data
WHERE device_id = 'X01' AND time > now() - interval '1 day'
GROUP BY bucket ORDER BY bucket;
````

 - 여기까지 간략히 알아보고 다음 준비에서 각 DB 선택 전략이나 이유에 대해 정리해 보기로 하자.


---

다음은 프로젝트에서 진행한 시스템 성능 개선, 구조적 방향성에 대해 알아보자.

 - DB Polling 방식 한계점 발견 및 성능 저하 가속화로 인한 Redis Delay Queue, kafka 방식으로 개선한 부분은 스케줄링을 통해 알림 데이터 전송처리 하지 않은 것을 탐색해 전송하는 처리를 한다. 이는 DB를 통해 IO 부하가 많이 생긴다.
 - 성능을 개선하기 위해 선택한 부분은 DB IO를 최소화하기 위한 방법으로 redis delay queue와 kafka를 선택했다. redis에 ZSET score 기반 데이터 삽입과 score를 통해 데이터를 꺼내 kafka로 비동기 처리해 성능을 최적화했다. 
 - 이후 고도화에 대해 레디스 클러스터링 고려 가능 클러스터링 진행 시 분산 락을 사용한 동시성 고려할 수 있다.





1분 안으로 트래픽이 집중되는 상황에서 프로모션의 완전한 진행으로 목표치인 7000명으로 제한한 프로모션을 진행 시 딱 7000명까지의 요청만 받고 저장해야 한다, 이외는 정원 초과 메시지를 발행한다는 목표 설정에 대한 시스템 설계에 대해 고려할 점은 다음과 같다.



1. 원자성 확보 - 정확한 목표치 인원에 대해 초과하지 않아야 한다.

2. 고속 처리와 빠른 응답 - 대량 트래픽 발생에 따라 쓰기 작업이 DB에 몰릴 경우 성능상 이슈가 생길 수 있다.

3. 이중 처리 방지 / 중복 체크 - 중복된 사용자가 참여하지 않도록 해야 한다.

4. 정확한 메시지 응답 처리 - 처리된 데이터에 따라 처리된 상태를 정확히 메시지로 반환해야 한다.



- 위 목표치를 달성하기 위해 redis를 사용해서 1차로 쓰기에 대한 작업을 진행한다. 이로 고속 처리 해결, redis에 쓰기 작업을 진행할 때 Rua script를 통해 이중처리나 원자성을 확보했다. 
 - 단일 스레드 단일 명령으로 진행하기 때문에 여러 명령을 한 번에 작업에 유기적으로 진행, 이중처리를 방지하기 위해 Lua script를 사용한 것이고 score와 유저정보를 통해 원자성을 확보했다. 
 - 차후 kafka를 통해 redis 데이터는 DB로 별도 저장하게 된다. 이때 redis의 데이터는 영구 저장이 되지 않는다. 
 - 영구 저장을 하기 위해서 kafka 메시지를 통해 Redis 데이터를 참여 인원을 끊어서 별도 참여자를 저장하는 테이블에 저장해서 IO 트래픽을 줄였고 저장된 참여자 정보는 쿠폰 서비스로 참여자 정보를 주고 쿠폰을 발급받게 해야 한다. 이때 kafka를 통해 메시지를 배치 처리 하게끔 진행했고 메시지 유실에 대한 문제는 DLQ로 해결했지만 메시지 이중 발송, 유실 시에 대한 대책과 참여자 저장하고 쿠폰 서비스에 데이터가 메시지로 발행 돼야 하기 때문에 OUTBOX 패턴을 적용했다. 
 - CDC까지는 사용하지 못했지만 사용자 저장 시 별도로 구성된 outbox와 사용자는 별도 테이블에 저장되고 hibernate.jdbc.batch_size 설정과 배치 사이즈에 따라 한 번에 저장되게 최적화했다. 이후 DB polling 방식을 통해서 kafka 배치로 쿠폰 서비스에 저장하게 했다.
